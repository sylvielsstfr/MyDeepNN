{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <!-- TITLE --> [BHPD1] - Regression with a Dense Network (DNN)\n",
    "<!-- DESC --> Simple example of a regression with LSST-DESC DC2 simulated data (BHPD)\n",
    "<!-- AUTHOR : Sylvie Dagoret (CNRS/IJCLab) -->\n",
    "\n",
    "\n",
    "- use fidle environnement `fidle23`\n",
    "\n",
    "## Objectives :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sylvielsstfr/MyDP0LSS/blob/main/myDP0.2/PhotoZ/MLScikitL_Estimator/02_MLscikitL_PhotoZSimple_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(z_spec,z_phot,slope=0.15):\n",
    "    \"\"\"\n",
    "    input : \n",
    "       - z_spec : spectroscopic redshift or true redshift\n",
    "       - z_phot : photo-z reedshift\n",
    "       - slope : slope of line defining the outliers  3 x sigma_z with sigma_z = 5%, so slope = 3 x 0.05 = 0.15 \n",
    "    \"\"\"\n",
    "    \n",
    "    mask = np.abs((z_phot - z_spec)/(1 + z_spec)) > slope\n",
    "    notmask = ~mask \n",
    "    \n",
    "    # Standard Deviation of the predicted redshifts compared to the data:\n",
    "    #-----------------------------------------------------------------\n",
    "    std_result = np.std((z_phot - z_spec)/(1 + z_spec), ddof=1)\n",
    "    print('Standard Deviation: %6.4f' % std_result)\n",
    "    \n",
    "\n",
    "    # Normalized MAD (Median Absolute Deviation):\n",
    "    #------------------------------------------\n",
    "    nmad = 1.48 * np.median(np.abs((z_phot - z_spec)/(1 + z_spec)))\n",
    "    print('Normalized MAD: %6.4f' % nmad)\n",
    "\n",
    "    # Percentage of delta-z > 0.15(1+z) outliers:\n",
    "    #-------------------------------------------\n",
    "    eta = np.sum(np.abs((z_phot - z_spec)/(1 + z_spec)) > 0.15)/len(z_spec)\n",
    "    print('Delta z >0.15(1+z) outliers: %6.3f percent' % (100.*eta))\n",
    "    \n",
    "    # Median offset (normalized by (1+z); i.e., bias:\n",
    "    #-----------------------------------------------\n",
    "    bias = np.median(((z_phot - z_spec)/(1 + z_spec)))\n",
    "    sigbias=std_result/np.sqrt(0.64*len(z_phot))\n",
    "    print('Median offset: %6.3f +/- %6.3f' % (bias,sigbias))\n",
    "    \n",
    "    \n",
    "     # overlay statistics with titles left-aligned and numbers right-aligned\n",
    "    stats_txt = '\\n'.join([\n",
    "        'NMAD  = {:0.2f}'.format(nmad),\n",
    "        'STDEV = {:0.2f}'.format(std_result),\n",
    "        'BIAS  = {:0.2f}'.format(bias),\n",
    "        'ETA   = {:0.2f}'.format(eta)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return nmad,std_result,bias,eta,stats_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - Import and init\n",
    "\n",
    "You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL :\n",
    "- 0 = all messages are logged (default)\n",
    "- 1 = INFO messages are not printed.\n",
    "- 2 = INFO and WARNING messages are not printed.\n",
    "- 3 = INFO , WARNING and ERROR messages are not printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 10:30:56.370524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div.warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: bold;\n",
       "    font-size: 1.1em;;\n",
       "    }\n",
       "\n",
       "\n",
       "\n",
       "div.nota {    \n",
       "    background-color: #DAFFDE;\n",
       "    border-left: 5px solid #92CC99;\n",
       "    padding: 0.5em;\n",
       "    }\n",
       "\n",
       "div.todo:before { content:url(data:image/svg+xml;base64,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);\n",
       "    float:left;\n",
       "    margin-right:20px;\n",
       "    margin-top:-20px;\n",
       "    margin-bottom:20px;\n",
       "}\n",
       "div.todo{\n",
       "    font-weight: bold;\n",
       "    font-size: 1.1em;\n",
       "    margin-top:40px;\n",
       "}\n",
       "div.todo ul{\n",
       "    margin: 0.2em;\n",
       "}\n",
       "div.todo li{\n",
       "    margin-left:60px;\n",
       "    margin-top:0;\n",
       "    margin-bottom:0;\n",
       "}\n",
       "\n",
       "div .comment{\n",
       "    font-size:0.8em;\n",
       "    color:#696969;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br>**FIDLE - Environment initialization**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version              : 2.0b56\n",
      "Run id               : BHPD1\n",
      "Run dir              : ./run/BHPD1\n",
      "Datasets dir         : /Users/dagoret/MacOSX/Fidle2023/fidle-tp/datasets-fidle\n",
      "Start time           : 10/04/23 10:31:11\n",
      "Hostname             : MacBook-Pro-de-admin.local (Darwin)\n",
      "Tensorflow log level : Warning + Error  (=1)\n",
      "Update keras cache   : False\n",
      "Save figs            : ./run/BHPD1/figs (False)\n",
      "tensorflow           : 2.12.0\n",
      "numpy                : 1.23.5\n",
      "sklearn              : 1.2.2\n",
      "matplotlib           : 3.7.1\n",
      "pandas               : 2.0.0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import h5py\n",
    "\n",
    "import fidle\n",
    "\n",
    "# Init Fidle environment\n",
    "run_id, run_dir, datasets_dir = fidle.init('BHPD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbosity during training : \n",
    "- 0 = silent\n",
    "- 1 = progress bar\n",
    "- 2 = one line per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_verbosity = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Override parameters (batch mode) - Just forget this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.override('fit_verbosity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Retrieve data\n",
    "\n",
    "### 2.1 - Option 1  : From Keras\n",
    "Boston housing is a famous historic dataset, so we can get it directly from [Keras datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_file_h5 = \"../data/test_dc2_training_9816.hdf5\"\n",
    "input_test_file_h5 = \"../data/test_dc2_validation_9816.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hftrain =  h5py.File(input_train_file_h5, 'r') \n",
    "hftest =  h5py.File(input_test_file_h5, 'r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(hf):\n",
    "    key_sel = list(hf.keys())[0]\n",
    "    group = hf.get(key_sel)\n",
    "    \n",
    "    mag_err_u_lsst = np.array(group.get(\"mag_err_u_lsst\"))\n",
    "    mag_err_g_lsst = np.array(group.get(\"mag_err_g_lsst\"))\n",
    "    mag_err_r_lsst = np.array(group.get(\"mag_err_r_lsst\"))\n",
    "    mag_err_i_lsst = np.array(group.get(\"mag_err_i_lsst\"))\n",
    "    mag_err_z_lsst = np.array(group.get(\"mag_err_z_lsst\"))\n",
    "    mag_err_y_lsst = np.array(group.get(\"mag_err_y_lsst\"))\n",
    "    mag_u_lsst =  np.array(group.get(\"mag_u_lsst\"))\n",
    "    mag_g_lsst =  np.array(group.get(\"mag_g_lsst\"))\n",
    "    mag_r_lsst =  np.array(group.get(\"mag_r_lsst\"))\n",
    "    mag_i_lsst =  np.array(group.get(\"mag_i_lsst\"))\n",
    "    mag_z_lsst =  np.array(group.get(\"mag_z_lsst\"))\n",
    "    mag_y_lsst =  np.array(group.get(\"mag_y_lsst\"))\n",
    "    redshift = np.array(group.get(\"redshift\"))\n",
    "    \n",
    "    data = np.vstack((mag_u_lsst, \n",
    "                    mag_g_lsst, \n",
    "                    mag_r_lsst, \n",
    "                    mag_i_lsst, \n",
    "                    mag_z_lsst,\n",
    "                    mag_y_lsst,\n",
    "                    redshift)) \n",
    "    data = data.T\n",
    "    indexes_bad = np.where(data[:,0]>40)[0]\n",
    "    datacut = np.delete(data,indexes_bad,axis=0)\n",
    "    features = datacut[:,0:-1]\n",
    "    targets = datacut[:,-1]\n",
    "    return features,targets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = getdata(hftrain)\n",
    "x_test,y_test = getdata(hftest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 - Data normalization\n",
    "**Note :** \n",
    " - All input data must be normalized, train and test.  \n",
    " - To do this we will **subtract the mean** and **divide by the standard deviation**.  \n",
    " - But test data should not be used in any way, even for normalization.  \n",
    " - The mean and the standard deviation will therefore only be calculated with the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x_train.mean()\n",
    "std  = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test  = (x_test  - mean) / std\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_test,  y_test  = np.array(x_test),  np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Build a model\n",
    "About informations about : \n",
    " - [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    " - [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    " - [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    " - [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(shape):\n",
    "  \n",
    "  model = keras.models.Sequential()\n",
    "  model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "  model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "  model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "  model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "  model.add(keras.layers.Dense(1, name='Output'))\n",
    "  \n",
    "  model.compile(optimizer = 'adam',\n",
    "                loss      = 'mse',\n",
    "                metrics   = ['mae', 'mse'] )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train the model\n",
    "### 5.1 - Get it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_n1 (Dense)            (None, 32)                224       \n",
      "                                                                 \n",
      " Dense_n2 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " Dense_n3 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,449\n",
      "Trainable params: 4,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=get_model_v1( (6,) )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# img=keras.utils.plot_model( model, to_file='./run/model.png', show_shapes=True, show_layer_names=True, dpi=96)\n",
    "# display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "476/476 [==============================] - 4s 6ms/step - loss: 0.1116 - mae: 0.2043 - mse: 0.1116 - val_loss: 0.0477 - val_mae: 0.1433 - val_mse: 0.0477\n",
      "Epoch 2/100\n",
      "476/476 [==============================] - 3s 6ms/step - loss: 0.0468 - mae: 0.1328 - mse: 0.0468 - val_loss: 0.0325 - val_mae: 0.1171 - val_mse: 0.0325\n",
      "Epoch 3/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0317 - mae: 0.1159 - mse: 0.0317 - val_loss: 0.0291 - val_mae: 0.1112 - val_mse: 0.0291\n",
      "Epoch 4/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0281 - mae: 0.1088 - mse: 0.0281 - val_loss: 0.0263 - val_mae: 0.1041 - val_mse: 0.0263\n",
      "Epoch 5/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0244 - mae: 0.1014 - mse: 0.0244 - val_loss: 0.0299 - val_mae: 0.1149 - val_mse: 0.0299\n",
      "Epoch 6/100\n",
      "476/476 [==============================] - 3s 5ms/step - loss: 0.0274 - mae: 0.1030 - mse: 0.0274 - val_loss: 0.0228 - val_mae: 0.0925 - val_mse: 0.0228\n",
      "Epoch 7/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0229 - mae: 0.0964 - mse: 0.0229 - val_loss: 0.0262 - val_mae: 0.1018 - val_mse: 0.0262\n",
      "Epoch 8/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0213 - mae: 0.0932 - mse: 0.0213 - val_loss: 0.0212 - val_mae: 0.0902 - val_mse: 0.0212\n",
      "Epoch 9/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0209 - mae: 0.0919 - mse: 0.0209 - val_loss: 0.0246 - val_mae: 0.1067 - val_mse: 0.0246\n",
      "Epoch 10/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0203 - mae: 0.0889 - mse: 0.0203 - val_loss: 0.0204 - val_mae: 0.0873 - val_mse: 0.0204\n",
      "Epoch 11/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0186 - mae: 0.0854 - mse: 0.0186 - val_loss: 0.0194 - val_mae: 0.0824 - val_mse: 0.0194\n",
      "Epoch 12/100\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0198 - mae: 0.0867 - mse: 0.0198 - val_loss: 0.0202 - val_mae: 0.0875 - val_mse: 0.0202\n",
      "Epoch 13/100\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0201 - mae: 0.0851 - mse: 0.0201 - val_loss: 0.0197 - val_mae: 0.0849 - val_mse: 0.0197\n",
      "Epoch 14/100\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0182 - mae: 0.0826 - mse: 0.0182 - val_loss: 0.0184 - val_mae: 0.0794 - val_mse: 0.0184\n",
      "Epoch 15/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0177 - mae: 0.0799 - mse: 0.0177 - val_loss: 0.0178 - val_mae: 0.0780 - val_mse: 0.0178\n",
      "Epoch 16/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0181 - mae: 0.0796 - mse: 0.0181 - val_loss: 0.0200 - val_mae: 0.0767 - val_mse: 0.0200\n",
      "Epoch 17/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0174 - mae: 0.0798 - mse: 0.0174 - val_loss: 0.0175 - val_mae: 0.0761 - val_mse: 0.0175\n",
      "Epoch 18/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0172 - mae: 0.0771 - mse: 0.0172 - val_loss: 0.0183 - val_mae: 0.0739 - val_mse: 0.0183\n",
      "Epoch 19/100\n",
      "476/476 [==============================] - 3s 6ms/step - loss: 0.0183 - mae: 0.0775 - mse: 0.0183 - val_loss: 0.0172 - val_mae: 0.0764 - val_mse: 0.0172\n",
      "Epoch 20/100\n",
      "476/476 [==============================] - 3s 5ms/step - loss: 0.0165 - mae: 0.0750 - mse: 0.0165 - val_loss: 0.0160 - val_mae: 0.0738 - val_mse: 0.0160\n",
      "Epoch 21/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0155 - mae: 0.0735 - mse: 0.0155 - val_loss: 0.0181 - val_mae: 0.0720 - val_mse: 0.0181\n",
      "Epoch 22/100\n",
      "476/476 [==============================] - 3s 5ms/step - loss: 0.0152 - mae: 0.0726 - mse: 0.0152 - val_loss: 0.0176 - val_mae: 0.0717 - val_mse: 0.0176\n",
      "Epoch 23/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0153 - mae: 0.0729 - mse: 0.0153 - val_loss: 0.0156 - val_mae: 0.0692 - val_mse: 0.0156\n",
      "Epoch 24/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0155 - mae: 0.0730 - mse: 0.0155 - val_loss: 0.0180 - val_mae: 0.0794 - val_mse: 0.0180\n",
      "Epoch 25/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0162 - mae: 0.0737 - mse: 0.0162 - val_loss: 0.0155 - val_mae: 0.0700 - val_mse: 0.0155\n",
      "Epoch 26/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0147 - mae: 0.0709 - mse: 0.0147 - val_loss: 0.0246 - val_mae: 0.0811 - val_mse: 0.0246\n",
      "Epoch 27/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0142 - mae: 0.0696 - mse: 0.0142 - val_loss: 0.0177 - val_mae: 0.0759 - val_mse: 0.0177\n",
      "Epoch 28/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0137 - mae: 0.0687 - mse: 0.0137 - val_loss: 0.0150 - val_mae: 0.0676 - val_mse: 0.0150\n",
      "Epoch 29/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0144 - mae: 0.0701 - mse: 0.0144 - val_loss: 0.0238 - val_mae: 0.0783 - val_mse: 0.0238\n",
      "Epoch 30/100\n",
      "476/476 [==============================] - 5s 10ms/step - loss: 0.0148 - mae: 0.0709 - mse: 0.0148 - val_loss: 0.0164 - val_mae: 0.0697 - val_mse: 0.0164\n",
      "Epoch 31/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0146 - mae: 0.0695 - mse: 0.0146 - val_loss: 0.0159 - val_mae: 0.0677 - val_mse: 0.0159\n",
      "Epoch 32/100\n",
      "476/476 [==============================] - 4s 9ms/step - loss: 0.0150 - mae: 0.0697 - mse: 0.0150 - val_loss: 0.0166 - val_mae: 0.0719 - val_mse: 0.0166\n",
      "Epoch 33/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0130 - mae: 0.0672 - mse: 0.0130 - val_loss: 0.0162 - val_mae: 0.0672 - val_mse: 0.0162\n",
      "Epoch 34/100\n",
      "476/476 [==============================] - 4s 9ms/step - loss: 0.0141 - mae: 0.0679 - mse: 0.0141 - val_loss: 0.0149 - val_mae: 0.0660 - val_mse: 0.0149\n",
      "Epoch 35/100\n",
      "476/476 [==============================] - 6s 12ms/step - loss: 0.0126 - mae: 0.0648 - mse: 0.0126 - val_loss: 0.0146 - val_mae: 0.0674 - val_mse: 0.0146\n",
      "Epoch 36/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0133 - mae: 0.0667 - mse: 0.0133 - val_loss: 0.0163 - val_mae: 0.0707 - val_mse: 0.0163\n",
      "Epoch 37/100\n",
      "476/476 [==============================] - 7s 14ms/step - loss: 0.0137 - mae: 0.0663 - mse: 0.0137 - val_loss: 0.0151 - val_mae: 0.0658 - val_mse: 0.0151\n",
      "Epoch 38/100\n",
      "476/476 [==============================] - 6s 12ms/step - loss: 0.0151 - mae: 0.0687 - mse: 0.0151 - val_loss: 0.0157 - val_mae: 0.0682 - val_mse: 0.0157\n",
      "Epoch 39/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0129 - mae: 0.0652 - mse: 0.0129 - val_loss: 0.0148 - val_mae: 0.0662 - val_mse: 0.0148\n",
      "Epoch 40/100\n",
      "476/476 [==============================] - 6s 12ms/step - loss: 0.0126 - mae: 0.0644 - mse: 0.0126 - val_loss: 0.0182 - val_mae: 0.0690 - val_mse: 0.0182\n",
      "Epoch 41/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0132 - mae: 0.0646 - mse: 0.0132 - val_loss: 0.0191 - val_mae: 0.0663 - val_mse: 0.0191\n",
      "Epoch 42/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0126 - mae: 0.0643 - mse: 0.0126 - val_loss: 0.0203 - val_mae: 0.0676 - val_mse: 0.0203\n",
      "Epoch 43/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0140 - mae: 0.0659 - mse: 0.0140 - val_loss: 0.0156 - val_mae: 0.0690 - val_mse: 0.0156\n",
      "Epoch 44/100\n",
      "476/476 [==============================] - 5s 10ms/step - loss: 0.0116 - mae: 0.0628 - mse: 0.0116 - val_loss: 0.0160 - val_mae: 0.0680 - val_mse: 0.0160\n",
      "Epoch 45/100\n",
      "476/476 [==============================] - 4s 9ms/step - loss: 0.0135 - mae: 0.0641 - mse: 0.0135 - val_loss: 0.0149 - val_mae: 0.0640 - val_mse: 0.0149\n",
      "Epoch 46/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0113 - mae: 0.0616 - mse: 0.0113 - val_loss: 0.0196 - val_mae: 0.0691 - val_mse: 0.0196\n",
      "Epoch 47/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0131 - mae: 0.0641 - mse: 0.0131 - val_loss: 0.0164 - val_mae: 0.0647 - val_mse: 0.0164\n",
      "Epoch 48/100\n",
      "476/476 [==============================] - 4s 7ms/step - loss: 0.0131 - mae: 0.0632 - mse: 0.0131 - val_loss: 0.0139 - val_mae: 0.0643 - val_mse: 0.0139\n",
      "Epoch 49/100\n",
      "476/476 [==============================] - 3s 6ms/step - loss: 0.0131 - mae: 0.0638 - mse: 0.0131 - val_loss: 0.0154 - val_mae: 0.0654 - val_mse: 0.0154\n",
      "Epoch 50/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0124 - mae: 0.0630 - mse: 0.0124 - val_loss: 0.0160 - val_mae: 0.0653 - val_mse: 0.0160\n",
      "Epoch 51/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0113 - mae: 0.0613 - mse: 0.0113 - val_loss: 0.0143 - val_mae: 0.0624 - val_mse: 0.0143\n",
      "Epoch 52/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0130 - mae: 0.0638 - mse: 0.0130 - val_loss: 0.0140 - val_mae: 0.0614 - val_mse: 0.0140\n",
      "Epoch 53/100\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0134 - mae: 0.0641 - mse: 0.0134 - val_loss: 0.0143 - val_mae: 0.0642 - val_mse: 0.0143\n",
      "Epoch 54/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0122 - mae: 0.0611 - mse: 0.0122 - val_loss: 0.0151 - val_mae: 0.0641 - val_mse: 0.0151\n",
      "Epoch 55/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0122 - mae: 0.0626 - mse: 0.0122 - val_loss: 0.0144 - val_mae: 0.0613 - val_mse: 0.0144\n",
      "Epoch 56/100\n",
      "476/476 [==============================] - 3s 6ms/step - loss: 0.0124 - mae: 0.0622 - mse: 0.0124 - val_loss: 0.0153 - val_mae: 0.0629 - val_mse: 0.0153\n",
      "Epoch 57/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0111 - mae: 0.0601 - mse: 0.0111 - val_loss: 0.0157 - val_mae: 0.0629 - val_mse: 0.0157\n",
      "Epoch 58/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0118 - mae: 0.0620 - mse: 0.0118 - val_loss: 0.0150 - val_mae: 0.0633 - val_mse: 0.0150\n",
      "Epoch 59/100\n",
      "476/476 [==============================] - 5s 10ms/step - loss: 0.0132 - mae: 0.0629 - mse: 0.0132 - val_loss: 0.0150 - val_mae: 0.0618 - val_mse: 0.0150\n",
      "Epoch 60/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0119 - mae: 0.0607 - mse: 0.0119 - val_loss: 0.0154 - val_mae: 0.0665 - val_mse: 0.0154\n",
      "Epoch 61/100\n",
      "476/476 [==============================] - 5s 10ms/step - loss: 0.0116 - mae: 0.0604 - mse: 0.0116 - val_loss: 0.0151 - val_mae: 0.0648 - val_mse: 0.0151\n",
      "Epoch 62/100\n",
      "476/476 [==============================] - 6s 12ms/step - loss: 0.0113 - mae: 0.0591 - mse: 0.0113 - val_loss: 0.0139 - val_mae: 0.0609 - val_mse: 0.0139\n",
      "Epoch 63/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0117 - mae: 0.0601 - mse: 0.0117 - val_loss: 0.0146 - val_mae: 0.0633 - val_mse: 0.0146\n",
      "Epoch 64/100\n",
      "476/476 [==============================] - 5s 10ms/step - loss: 0.0116 - mae: 0.0601 - mse: 0.0116 - val_loss: 0.0183 - val_mae: 0.0709 - val_mse: 0.0183\n",
      "Epoch 65/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0123 - mae: 0.0604 - mse: 0.0123 - val_loss: 0.0158 - val_mae: 0.0634 - val_mse: 0.0158\n",
      "Epoch 66/100\n",
      "476/476 [==============================] - 4s 9ms/step - loss: 0.0111 - mae: 0.0586 - mse: 0.0111 - val_loss: 0.0143 - val_mae: 0.0614 - val_mse: 0.0143\n",
      "Epoch 67/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0114 - mae: 0.0593 - mse: 0.0114 - val_loss: 0.0150 - val_mae: 0.0605 - val_mse: 0.0150\n",
      "Epoch 68/100\n",
      "476/476 [==============================] - 4s 9ms/step - loss: 0.0124 - mae: 0.0605 - mse: 0.0124 - val_loss: 0.0166 - val_mae: 0.0649 - val_mse: 0.0166\n",
      "Epoch 69/100\n",
      "476/476 [==============================] - 4s 9ms/step - loss: 0.0109 - mae: 0.0578 - mse: 0.0109 - val_loss: 0.0138 - val_mae: 0.0610 - val_mse: 0.0138\n",
      "Epoch 70/100\n",
      "476/476 [==============================] - 3s 6ms/step - loss: 0.0120 - mae: 0.0593 - mse: 0.0120 - val_loss: 0.0173 - val_mae: 0.0617 - val_mse: 0.0173\n",
      "Epoch 71/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0105 - mae: 0.0579 - mse: 0.0105 - val_loss: 0.0194 - val_mae: 0.0694 - val_mse: 0.0194\n",
      "Epoch 72/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0117 - mae: 0.0592 - mse: 0.0117 - val_loss: 0.0153 - val_mae: 0.0659 - val_mse: 0.0153\n",
      "Epoch 73/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0113 - mae: 0.0585 - mse: 0.0113 - val_loss: 0.0164 - val_mae: 0.0661 - val_mse: 0.0164\n",
      "Epoch 74/100\n",
      "476/476 [==============================] - 4s 9ms/step - loss: 0.0107 - mae: 0.0576 - mse: 0.0107 - val_loss: 0.0197 - val_mae: 0.0653 - val_mse: 0.0197\n",
      "Epoch 75/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0108 - mae: 0.0575 - mse: 0.0108 - val_loss: 0.0143 - val_mae: 0.0587 - val_mse: 0.0143\n",
      "Epoch 76/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0113 - mae: 0.0583 - mse: 0.0113 - val_loss: 0.0159 - val_mae: 0.0677 - val_mse: 0.0159\n",
      "Epoch 77/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0110 - mae: 0.0575 - mse: 0.0110 - val_loss: 0.0153 - val_mae: 0.0634 - val_mse: 0.0153\n",
      "Epoch 78/100\n",
      "476/476 [==============================] - 3s 6ms/step - loss: 0.0107 - mae: 0.0569 - mse: 0.0107 - val_loss: 0.0147 - val_mae: 0.0590 - val_mse: 0.0147\n",
      "Epoch 79/100\n",
      "476/476 [==============================] - 3s 6ms/step - loss: 0.0113 - mae: 0.0571 - mse: 0.0113 - val_loss: 0.0148 - val_mae: 0.0610 - val_mse: 0.0148\n",
      "Epoch 80/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0116 - mae: 0.0587 - mse: 0.0116 - val_loss: 0.0138 - val_mae: 0.0598 - val_mse: 0.0138\n",
      "Epoch 81/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0101 - mae: 0.0569 - mse: 0.0101 - val_loss: 0.0141 - val_mae: 0.0625 - val_mse: 0.0141\n",
      "Epoch 82/100\n",
      "476/476 [==============================] - 3s 6ms/step - loss: 0.0120 - mae: 0.0588 - mse: 0.0120 - val_loss: 0.0138 - val_mae: 0.0609 - val_mse: 0.0138\n",
      "Epoch 83/100\n",
      "476/476 [==============================] - 4s 9ms/step - loss: 0.0111 - mae: 0.0573 - mse: 0.0111 - val_loss: 0.0168 - val_mae: 0.0625 - val_mse: 0.0168\n",
      "Epoch 84/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0108 - mae: 0.0561 - mse: 0.0108 - val_loss: 0.0170 - val_mae: 0.0662 - val_mse: 0.0170\n",
      "Epoch 85/100\n",
      "476/476 [==============================] - 5s 11ms/step - loss: 0.0100 - mae: 0.0562 - mse: 0.0100 - val_loss: 0.0143 - val_mae: 0.0590 - val_mse: 0.0143\n",
      "Epoch 86/100\n",
      "476/476 [==============================] - 4s 8ms/step - loss: 0.0109 - mae: 0.0565 - mse: 0.0109 - val_loss: 0.0156 - val_mae: 0.0655 - val_mse: 0.0156\n",
      "Epoch 87/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0110 - mae: 0.0582 - mse: 0.0110 - val_loss: 0.0148 - val_mae: 0.0603 - val_mse: 0.0148\n",
      "Epoch 88/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0113 - mae: 0.0570 - mse: 0.0113 - val_loss: 0.0147 - val_mae: 0.0613 - val_mse: 0.0147\n",
      "Epoch 89/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0101 - mae: 0.0562 - mse: 0.0101 - val_loss: 0.0144 - val_mae: 0.0595 - val_mse: 0.0144\n",
      "Epoch 90/100\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0104 - mae: 0.0558 - mse: 0.0104 - val_loss: 0.0156 - val_mae: 0.0623 - val_mse: 0.0156\n",
      "Epoch 91/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0103 - mae: 0.0566 - mse: 0.0103 - val_loss: 0.0173 - val_mae: 0.0641 - val_mse: 0.0173\n",
      "Epoch 92/100\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0105 - mae: 0.0575 - mse: 0.0105 - val_loss: 0.0172 - val_mae: 0.0631 - val_mse: 0.0172\n",
      "Epoch 93/100\n",
      "476/476 [==============================] - 2s 5ms/step - loss: 0.0103 - mae: 0.0556 - mse: 0.0103 - val_loss: 0.0160 - val_mae: 0.0638 - val_mse: 0.0160\n",
      "Epoch 94/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0106 - mae: 0.0568 - mse: 0.0106 - val_loss: 0.0151 - val_mae: 0.0587 - val_mse: 0.0151\n",
      "Epoch 95/100\n",
      "476/476 [==============================] - 3s 7ms/step - loss: 0.0098 - mae: 0.0553 - mse: 0.0098 - val_loss: 0.0137 - val_mae: 0.0588 - val_mse: 0.0137\n",
      "Epoch 96/100\n",
      "476/476 [==============================] - ETA: 0s - loss: 0.0108 - mae: 0.0567 - mse: 0.0108"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs          = 100,\n",
    "                    batch_size      = 20,\n",
    "                    verbose         = fit_verbosity,\n",
    "                    validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6 - Evaluate\n",
    "### 6.1 - Model evaluation\n",
    "MAE =  Mean Absolute Error (between the labels and predictions)  \n",
    "A mae equal to 3 represents an average error in prediction of $3k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Training history\n",
    "What was the best result during our training ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=history.history)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.scrawler.history( history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                        'MAE' :['mae', 'val_mae'],\n",
    "                        'LOSS':['loss','val_loss']}, save_as='01-history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Make a prediction\n",
    "The data must be normalized with the parameters (mean, std) previously used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data=x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict( my_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_spec = y_test\n",
    "z_phot = predictions.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt = get_stats(z_spec,z_phot,slope=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are matplotlib.patch.Patch properties\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(zmin=0,zmax=3,zstep=0.05,slope=0.15):\n",
    "    x = np.arange(zmin,zmax,zstep)\n",
    "    outlier_upper = x + slope*(1+x)\n",
    "    outlier_lower = x - slope*(1+x)\n",
    "    return x,outlier_upper,outlier_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,outlier_upper,outlier_lower = plot_lines(zmin=0,zmax=3,zstep=0.05,slope=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,10))\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "ax.scatter(z_spec,z_phot,marker='.',c=\"b\",s=30,alpha=0.1)\n",
    "ax.text(0.05, 0.95, stats_txt, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "ax.set_xlabel(\"$z_{spec}$ (true target value)\")\n",
    "ax.set_ylabel(\"$z_{phot}$ (predicted target value)\")\n",
    "ax.set_title(\"DNN :photometric redshift vs spectroscopic redshift\")\n",
    "ax.plot(x,outlier_upper,'k:')\n",
    "ax.plot(x,outlier_lower,'k:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fidle23",
   "language": "python",
   "name": "fidle23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3929042cc22c1274d74e3e946c52b845b57cb6d84f2d591ffe0519b38e4896d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
