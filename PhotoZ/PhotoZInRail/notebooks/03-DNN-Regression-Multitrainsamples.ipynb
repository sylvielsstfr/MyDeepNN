{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <!-- TITLE --> [BHPD1] - Regression with a Dense Network (DNN)\n",
    "<!-- DESC --> Simple example of a regression with LSST-DESC DC2 simulated data (BHPD)\n",
    "<!-- AUTHOR : Sylvie Dagoret (CNRS/IJCLab) -->\n",
    "\n",
    "\n",
    "- use fidle environnement `fidle23`\n",
    "\n",
    "## Objectives :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sylvielsstfr/MyDP0LSS/blob/main/myDP0.2/PhotoZ/MLScikitL_Estimator/02_MLscikitL_PhotoZSimple_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(z_spec,z_phot,slope=0.15):\n",
    "    \"\"\"\n",
    "    input : \n",
    "       - z_spec : spectroscopic redshift or true redshift\n",
    "       - z_phot : photo-z reedshift\n",
    "       - slope : slope of line defining the outliers  3 x sigma_z with sigma_z = 5%, so slope = 3 x 0.05 = 0.15 \n",
    "    \"\"\"\n",
    "    \n",
    "    mask = np.abs((z_phot - z_spec)/(1 + z_spec)) > slope\n",
    "    notmask = ~mask \n",
    "    \n",
    "    # Standard Deviation of the predicted redshifts compared to the data:\n",
    "    #-----------------------------------------------------------------\n",
    "    std_result = np.std((z_phot - z_spec)/(1 + z_spec), ddof=1)\n",
    "    print('Standard Deviation: %6.4f' % std_result)\n",
    "    \n",
    "\n",
    "    # Normalized MAD (Median Absolute Deviation):\n",
    "    #------------------------------------------\n",
    "    nmad = 1.48 * np.median(np.abs((z_phot - z_spec)/(1 + z_spec)))\n",
    "    print('Normalized MAD: %6.4f' % nmad)\n",
    "\n",
    "    # Percentage of delta-z > 0.15(1+z) outliers:\n",
    "    #-------------------------------------------\n",
    "    eta = np.sum(np.abs((z_phot - z_spec)/(1 + z_spec)) > 0.15)/len(z_spec)\n",
    "    print('Delta z >0.15(1+z) outliers: %6.3f percent' % (100.*eta))\n",
    "    \n",
    "    # Median offset (normalized by (1+z); i.e., bias:\n",
    "    #-----------------------------------------------\n",
    "    bias = np.median(((z_phot - z_spec)/(1 + z_spec)))\n",
    "    sigbias=std_result/np.sqrt(0.64*len(z_phot))\n",
    "    print('Median offset: %6.3f +/- %6.3f' % (bias,sigbias))\n",
    "    \n",
    "    \n",
    "     # overlay statistics with titles left-aligned and numbers right-aligned\n",
    "    stats_txt = '\\n'.join([\n",
    "        'NMAD  = {:0.2f}'.format(nmad),\n",
    "        'STDEV = {:0.2f}'.format(std_result),\n",
    "        'BIAS  = {:0.2f}'.format(bias),\n",
    "        'ETA   = {:0.2f}'.format(eta)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return nmad,std_result,bias,eta,stats_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - Import and init\n",
    "\n",
    "You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL :\n",
    "- 0 = all messages are logged (default)\n",
    "- 1 = INFO messages are not printed.\n",
    "- 2 = INFO and WARNING messages are not printed.\n",
    "- 3 = INFO , WARNING and ERROR messages are not printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import h5py\n",
    "\n",
    "import fidle\n",
    "\n",
    "# Init Fidle environment\n",
    "run_id, run_dir, datasets_dir = fidle.init('BHPD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbosity during training : \n",
    "- 0 = silent\n",
    "- 1 = progress bar\n",
    "- 2 = one line per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_verbosity = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Override parameters (batch mode) - Just forget this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.override('fit_verbosity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Retrieve data\n",
    "\n",
    "### 2.1 - Option 1  : From Keras\n",
    "Boston housing is a famous historic dataset, so we can get it directly from [Keras datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_file_h5 = \"../data/test_dc2_training_9816.hdf5\"\n",
    "input_test_file_h5 = \"../data/test_dc2_validation_9816.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hftrain =  h5py.File(input_train_file_h5, 'r') \n",
    "hftest =  h5py.File(input_test_file_h5, 'r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(hf):\n",
    "    key_sel = list(hf.keys())[0]\n",
    "    group = hf.get(key_sel)\n",
    "    \n",
    "    mag_err_u_lsst = np.array(group.get(\"mag_err_u_lsst\"))\n",
    "    mag_err_g_lsst = np.array(group.get(\"mag_err_g_lsst\"))\n",
    "    mag_err_r_lsst = np.array(group.get(\"mag_err_r_lsst\"))\n",
    "    mag_err_i_lsst = np.array(group.get(\"mag_err_i_lsst\"))\n",
    "    mag_err_z_lsst = np.array(group.get(\"mag_err_z_lsst\"))\n",
    "    mag_err_y_lsst = np.array(group.get(\"mag_err_y_lsst\"))\n",
    "    mag_u_lsst =  np.array(group.get(\"mag_u_lsst\"))\n",
    "    mag_g_lsst =  np.array(group.get(\"mag_g_lsst\"))\n",
    "    mag_r_lsst =  np.array(group.get(\"mag_r_lsst\"))\n",
    "    mag_i_lsst =  np.array(group.get(\"mag_i_lsst\"))\n",
    "    mag_z_lsst =  np.array(group.get(\"mag_z_lsst\"))\n",
    "    mag_y_lsst =  np.array(group.get(\"mag_y_lsst\"))\n",
    "    redshift = np.array(group.get(\"redshift\"))\n",
    "    \n",
    "    data = np.vstack((mag_u_lsst, \n",
    "                    mag_g_lsst, \n",
    "                    mag_r_lsst, \n",
    "                    mag_i_lsst, \n",
    "                    mag_z_lsst,\n",
    "                    mag_y_lsst,\n",
    "                    redshift)) \n",
    "    data = data.T\n",
    "    indexes_bad = np.where(data[:,0]>40)[0]\n",
    "    datacut = np.delete(data,indexes_bad,axis=0)\n",
    "    features = datacut[:,0:-1]\n",
    "    targets = datacut[:,-1]\n",
    "    return features,targets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdataerror(hf):\n",
    "    key_sel = list(hf.keys())[0]\n",
    "    group = hf.get(key_sel)\n",
    "    \n",
    "    mag_err_u_lsst = np.array(group.get(\"mag_err_u_lsst\"))\n",
    "    mag_err_g_lsst = np.array(group.get(\"mag_err_g_lsst\"))\n",
    "    mag_err_r_lsst = np.array(group.get(\"mag_err_r_lsst\"))\n",
    "    mag_err_i_lsst = np.array(group.get(\"mag_err_i_lsst\"))\n",
    "    mag_err_z_lsst = np.array(group.get(\"mag_err_z_lsst\"))\n",
    "    mag_err_y_lsst = np.array(group.get(\"mag_err_y_lsst\"))\n",
    "    mag_u_lsst =  np.array(group.get(\"mag_u_lsst\"))\n",
    "    mag_g_lsst =  np.array(group.get(\"mag_g_lsst\"))\n",
    "    mag_r_lsst =  np.array(group.get(\"mag_r_lsst\"))\n",
    "    mag_i_lsst =  np.array(group.get(\"mag_i_lsst\"))\n",
    "    mag_z_lsst =  np.array(group.get(\"mag_z_lsst\"))\n",
    "    mag_y_lsst =  np.array(group.get(\"mag_y_lsst\"))\n",
    "    redshift = np.array(group.get(\"redshift\"))\n",
    "    \n",
    "    data = np.vstack((\n",
    "                    mag_u_lsst, \n",
    "                    mag_g_lsst, \n",
    "                    mag_r_lsst, \n",
    "                    mag_i_lsst, \n",
    "                    mag_z_lsst,\n",
    "                    mag_y_lsst,\n",
    "                    mag_err_u_lsst, \n",
    "                    mag_err_g_lsst, \n",
    "                    mag_err_r_lsst, \n",
    "                    mag_err_i_lsst, \n",
    "                    mag_err_z_lsst,\n",
    "                    mag_err_y_lsst,\n",
    "                    redshift)) \n",
    "    data = data.T\n",
    "    indexes_bad = np.where(data[:,0]>40)[0]\n",
    "    datacut = np.delete(data,indexes_bad,axis=0)\n",
    "    features = datacut[:,0:6]\n",
    "    errors = datacut[:,6:-1]\n",
    "    targets = datacut[:,-1]\n",
    "    return features,errors,targets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0,x_train_error0,y_train = getdataerror(hftrain)\n",
    "x_test0,x_test_error0,y_test = getdataerror(hftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBANDS = x_train0.shape[1]\n",
    "NSAMPLES = x_train0.shape[0]\n",
    "NSIM= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulatesamples(x,ex,nsim=NSIM):\n",
    "    nsamples = x.shape[0]\n",
    "    nbands = x.shape[1]\n",
    "    \n",
    "    all_samples = np.zeros((nsamples,nbands,nsim))\n",
    "    for idx in range(nbands):\n",
    "        mean = x[:,idx]\n",
    "        sig = ex[:,idx]\n",
    "        simdata = np.random.normal(mean, sig, (nsim,nsamples))\n",
    "        all_samples[:,idx,: ] = simdata.T\n",
    "    return all_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = simulatesamples(x_train0,x_train_error0,NSIM)\n",
    "test_data = simulatesamples(x_test0,x_test_error0,NSIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 - Data normalization\n",
    "- do it on first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_data[:,:,0],axis=0)\n",
    "std  = np.std(train_data[:,:,0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0_norm = (x_train0-mean)/std\n",
    "x_test0_norm = (x_test0-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_train_norm = []\n",
    "all_x_test_norm = []\n",
    "for isim in range(NSIM):\n",
    "    x_train_norm = (train_data[:,:,isim] - mean)/std\n",
    "    x_test_norm = (test_data[:,:,isim] - mean)/std\n",
    "    all_x_train_norm.append(x_train_norm) \n",
    "    all_x_test_norm.append(x_test_norm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Build a model\n",
    "About informations about : \n",
    " - [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    " - [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    " - [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    " - [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(shape):\n",
    "  \n",
    "  model = keras.models.Sequential()\n",
    "  model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "  model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "  model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "  model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "  model.add(keras.layers.Dense(1, name='Output'))\n",
    "  \n",
    "  model.compile(optimizer = 'adam',\n",
    "                loss      = 'mse',\n",
    "                metrics   = ['mae', 'mse'] )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train the model\n",
    "### 5.1 - Get it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = []\n",
    "for isim in range(NSIM):\n",
    "    model=get_model_v1( (NBANDS,))\n",
    "    all_models.append(model)\n",
    "\n",
    "all_models[0].summary()\n",
    "\n",
    "# img=keras.utils.plot_model( model, to_file='./run/model.png', show_shapes=True, show_layer_names=True, dpi=96)\n",
    "# display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_history = []\n",
    "for isim,the_model in enumerate(all_models):\n",
    "    x_train = all_x_train_norm[isim] \n",
    "    the_history = the_model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs          = 50,\n",
    "                    batch_size      = 20,\n",
    "                    verbose         = fit_verbosity,\n",
    "                    validation_data = (x_test0_norm, y_test))\n",
    "    all_history.append(the_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6 - Evaluate\n",
    "### 6.1 - Model evaluation\n",
    "MAE =  Mean Absolute Error (between the labels and predictions)  \n",
    "A mae equal to 3 represents an average error in prediction of $3k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Training history\n",
    "What was the best result during our training ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=history.history)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.scrawler.history( history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                        'MAE' :['mae', 'val_mae'],\n",
    "                        'LOSS':['loss','val_loss']}, save_as='01-history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Make a prediction\n",
    "The data must be normalized with the parameters (mean, std) previously used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data=x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict( my_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_spec = y_test\n",
    "z_phot = predictions.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt = get_stats(z_spec,z_phot,slope=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are matplotlib.patch.Patch properties\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(zmin=0,zmax=3,zstep=0.05,slope=0.15):\n",
    "    x = np.arange(zmin,zmax,zstep)\n",
    "    outlier_upper = x + slope*(1+x)\n",
    "    outlier_lower = x - slope*(1+x)\n",
    "    return x,outlier_upper,outlier_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,outlier_upper,outlier_lower = plot_lines(zmin=0,zmax=3,zstep=0.05,slope=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,10))\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "ax.scatter(z_spec,z_phot,marker='.',c=\"b\",s=30,alpha=0.1)\n",
    "ax.text(0.05, 0.95, stats_txt, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "ax.set_xlabel(\"$z_{spec}$ (true target value)\")\n",
    "ax.set_ylabel(\"$z_{phot}$ (predicted target value)\")\n",
    "ax.set_title(\"DNN :photometric redshift vs spectroscopic redshift\")\n",
    "ax.plot(x,outlier_upper,'k:')\n",
    "ax.plot(x,outlier_lower,'k:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fidle23",
   "language": "python",
   "name": "fidle23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3929042cc22c1274d74e3e946c52b845b57cb6d84f2d591ffe0519b38e4896d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
