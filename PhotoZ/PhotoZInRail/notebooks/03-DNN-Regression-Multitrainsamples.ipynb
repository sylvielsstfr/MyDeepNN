{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <!-- TITLE --> [BHPD1] - Regression with a Dense Network (DNN)\n",
    "<!-- DESC --> Simple example of a regression with LSST-DESC DC2 simulated data (BHPD)\n",
    "<!-- AUTHOR : Sylvie Dagoret (CNRS/IJCLab) -->\n",
    "\n",
    "\n",
    "- use fidle environnement `fidle23`\n",
    "\n",
    "## Objectives :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sylvielsstfr/MyDP0LSS/blob/main/myDP0.2/PhotoZ/MLScikitL_Estimator/02_MLscikitL_PhotoZSimple_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(z_spec,z_phot,slope=0.15):\n",
    "    \"\"\"\n",
    "    input : \n",
    "       - z_spec : spectroscopic redshift or true redshift\n",
    "       - z_phot : photo-z reedshift\n",
    "       - slope : slope of line defining the outliers  3 x sigma_z with sigma_z = 5%, so slope = 3 x 0.05 = 0.15 \n",
    "    \"\"\"\n",
    "    \n",
    "    mask = np.abs((z_phot - z_spec)/(1 + z_spec)) > slope\n",
    "    notmask = ~mask \n",
    "    \n",
    "    # Standard Deviation of the predicted redshifts compared to the data:\n",
    "    #-----------------------------------------------------------------\n",
    "    std_result = np.std((z_phot - z_spec)/(1 + z_spec), ddof=1)\n",
    "    print('Standard Deviation: %6.4f' % std_result)\n",
    "    \n",
    "\n",
    "    # Normalized MAD (Median Absolute Deviation):\n",
    "    #------------------------------------------\n",
    "    nmad = 1.48 * np.median(np.abs((z_phot - z_spec)/(1 + z_spec)))\n",
    "    print('Normalized MAD: %6.4f' % nmad)\n",
    "\n",
    "    # Percentage of delta-z > 0.15(1+z) outliers:\n",
    "    #-------------------------------------------\n",
    "    eta = np.sum(np.abs((z_phot - z_spec)/(1 + z_spec)) > 0.15)/len(z_spec)\n",
    "    print('Delta z >0.15(1+z) outliers: %6.3f percent' % (100.*eta))\n",
    "    \n",
    "    # Median offset (normalized by (1+z); i.e., bias:\n",
    "    #-----------------------------------------------\n",
    "    bias = np.median(((z_phot - z_spec)/(1 + z_spec)))\n",
    "    sigbias=std_result/np.sqrt(0.64*len(z_phot))\n",
    "    print('Median offset: %6.3f +/- %6.3f' % (bias,sigbias))\n",
    "    \n",
    "    \n",
    "     # overlay statistics with titles left-aligned and numbers right-aligned\n",
    "    stats_txt = '\\n'.join([\n",
    "        'NMAD  = {:0.2f}'.format(nmad),\n",
    "        'STDEV = {:0.2f}'.format(std_result),\n",
    "        'BIAS  = {:0.2f}'.format(bias),\n",
    "        'ETA   = {:0.2f}'.format(eta)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return nmad,std_result,bias,eta,stats_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - Import and init\n",
    "\n",
    "You can also adjust the verbosity by changing the value of TF_CPP_MIN_LOG_LEVEL :\n",
    "- 0 = all messages are logged (default)\n",
    "- 1 = INFO messages are not printed.\n",
    "- 2 = INFO and WARNING messages are not printed.\n",
    "- 3 = INFO , WARNING and ERROR messages are not printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 21:43:44.513027: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div.warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: bold;\n",
       "    font-size: 1.1em;;\n",
       "    }\n",
       "\n",
       "\n",
       "\n",
       "div.nota {    \n",
       "    background-color: #DAFFDE;\n",
       "    border-left: 5px solid #92CC99;\n",
       "    padding: 0.5em;\n",
       "    }\n",
       "\n",
       "div.todo:before { content:url(data:image/svg+xml;base64,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);\n",
       "    float:left;\n",
       "    margin-right:20px;\n",
       "    margin-top:-20px;\n",
       "    margin-bottom:20px;\n",
       "}\n",
       "div.todo{\n",
       "    font-weight: bold;\n",
       "    font-size: 1.1em;\n",
       "    margin-top:40px;\n",
       "}\n",
       "div.todo ul{\n",
       "    margin: 0.2em;\n",
       "}\n",
       "div.todo li{\n",
       "    margin-left:60px;\n",
       "    margin-top:0;\n",
       "    margin-bottom:0;\n",
       "}\n",
       "\n",
       "div .comment{\n",
       "    font-size:0.8em;\n",
       "    color:#696969;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br>**FIDLE - Environment initialization**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version              : 2.0b56\n",
      "Run id               : BHPD1\n",
      "Run dir              : ./run/BHPD1\n",
      "Datasets dir         : /Users/dagoret/MacOSX/Fidle2023/fidle-tp/datasets-fidle\n",
      "Start time           : 20/04/23 21:43:49\n",
      "Hostname             : MacBook-Pro-de-admin.local (Darwin)\n",
      "Tensorflow log level : Warning + Error  (=1)\n",
      "Update keras cache   : False\n",
      "Save figs            : ./run/BHPD1/figs (False)\n",
      "tensorflow           : 2.12.0\n",
      "numpy                : 1.23.5\n",
      "sklearn              : 1.2.2\n",
      "matplotlib           : 3.7.1\n",
      "pandas               : 2.0.0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import h5py\n",
    "\n",
    "import fidle\n",
    "\n",
    "# Init Fidle environment\n",
    "run_id, run_dir, datasets_dir = fidle.init('BHPD1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbosity during training : \n",
    "- 0 = silent\n",
    "- 1 = progress bar\n",
    "- 2 = one line per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_verbosity = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Override parameters (batch mode) - Just forget this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.override('fit_verbosity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Retrieve data\n",
    "\n",
    "### 2.1 - Option 1  : From Keras\n",
    "Boston housing is a famous historic dataset, so we can get it directly from [Keras datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_file_h5 = \"../data/test_dc2_training_9816.hdf5\"\n",
    "input_test_file_h5 = \"../data/test_dc2_validation_9816.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hftrain =  h5py.File(input_train_file_h5, 'r') \n",
    "hftest =  h5py.File(input_test_file_h5, 'r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(hf):\n",
    "    key_sel = list(hf.keys())[0]\n",
    "    group = hf.get(key_sel)\n",
    "    \n",
    "    mag_err_u_lsst = np.array(group.get(\"mag_err_u_lsst\"))\n",
    "    mag_err_g_lsst = np.array(group.get(\"mag_err_g_lsst\"))\n",
    "    mag_err_r_lsst = np.array(group.get(\"mag_err_r_lsst\"))\n",
    "    mag_err_i_lsst = np.array(group.get(\"mag_err_i_lsst\"))\n",
    "    mag_err_z_lsst = np.array(group.get(\"mag_err_z_lsst\"))\n",
    "    mag_err_y_lsst = np.array(group.get(\"mag_err_y_lsst\"))\n",
    "    mag_u_lsst =  np.array(group.get(\"mag_u_lsst\"))\n",
    "    mag_g_lsst =  np.array(group.get(\"mag_g_lsst\"))\n",
    "    mag_r_lsst =  np.array(group.get(\"mag_r_lsst\"))\n",
    "    mag_i_lsst =  np.array(group.get(\"mag_i_lsst\"))\n",
    "    mag_z_lsst =  np.array(group.get(\"mag_z_lsst\"))\n",
    "    mag_y_lsst =  np.array(group.get(\"mag_y_lsst\"))\n",
    "    redshift = np.array(group.get(\"redshift\"))\n",
    "    \n",
    "    data = np.vstack((mag_u_lsst, \n",
    "                    mag_g_lsst, \n",
    "                    mag_r_lsst, \n",
    "                    mag_i_lsst, \n",
    "                    mag_z_lsst,\n",
    "                    mag_y_lsst,\n",
    "                    redshift)) \n",
    "    data = data.T\n",
    "    indexes_bad = np.where(data[:,0]>40)[0]\n",
    "    datacut = np.delete(data,indexes_bad,axis=0)\n",
    "    features = datacut[:,0:-1]\n",
    "    targets = datacut[:,-1]\n",
    "    return features,targets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdataerror(hf):\n",
    "    key_sel = list(hf.keys())[0]\n",
    "    group = hf.get(key_sel)\n",
    "    \n",
    "    mag_err_u_lsst = np.array(group.get(\"mag_err_u_lsst\"))\n",
    "    mag_err_g_lsst = np.array(group.get(\"mag_err_g_lsst\"))\n",
    "    mag_err_r_lsst = np.array(group.get(\"mag_err_r_lsst\"))\n",
    "    mag_err_i_lsst = np.array(group.get(\"mag_err_i_lsst\"))\n",
    "    mag_err_z_lsst = np.array(group.get(\"mag_err_z_lsst\"))\n",
    "    mag_err_y_lsst = np.array(group.get(\"mag_err_y_lsst\"))\n",
    "    mag_u_lsst =  np.array(group.get(\"mag_u_lsst\"))\n",
    "    mag_g_lsst =  np.array(group.get(\"mag_g_lsst\"))\n",
    "    mag_r_lsst =  np.array(group.get(\"mag_r_lsst\"))\n",
    "    mag_i_lsst =  np.array(group.get(\"mag_i_lsst\"))\n",
    "    mag_z_lsst =  np.array(group.get(\"mag_z_lsst\"))\n",
    "    mag_y_lsst =  np.array(group.get(\"mag_y_lsst\"))\n",
    "    redshift = np.array(group.get(\"redshift\"))\n",
    "    \n",
    "    data = np.vstack((\n",
    "                    mag_u_lsst, \n",
    "                    mag_g_lsst, \n",
    "                    mag_r_lsst, \n",
    "                    mag_i_lsst, \n",
    "                    mag_z_lsst,\n",
    "                    mag_y_lsst,\n",
    "                    mag_err_u_lsst, \n",
    "                    mag_err_g_lsst, \n",
    "                    mag_err_r_lsst, \n",
    "                    mag_err_i_lsst, \n",
    "                    mag_err_z_lsst,\n",
    "                    mag_err_y_lsst,\n",
    "                    redshift)) \n",
    "    data = data.T\n",
    "    indexes_bad = np.where(data[:,0]>40)[0]\n",
    "    datacut = np.delete(data,indexes_bad,axis=0)\n",
    "    features = datacut[:,0:6]\n",
    "    errors = datacut[:,6:-1]\n",
    "    targets = datacut[:,-1]\n",
    "    return features,errors,targets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0,x_train_error0,y_train = getdataerror(hftrain)\n",
    "x_test0,x_test_error0,y_test = getdataerror(hftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBANDS = x_train0.shape[1]\n",
    "NSAMPLES = x_train0.shape[0]\n",
    "NSIM= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulatesamples(x,ex,nsim=NSIM):\n",
    "    nsamples = x.shape[0]\n",
    "    nbands = x.shape[1]\n",
    "    \n",
    "    all_samples = np.zeros((nsamples,nbands,nsim))\n",
    "    for idx in range(nbands):\n",
    "        mean = x[:,idx]\n",
    "        sig = ex[:,idx]\n",
    "        simdata = np.random.normal(mean, sig, (nsim,nsamples))\n",
    "        all_samples[:,idx,: ] = simdata.T\n",
    "    return all_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = simulatesamples(x_train0,x_train_error0,NSIM)\n",
    "test_data = simulatesamples(x_test0,x_test_error0,NSIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 - Data normalization\n",
    "- do it on first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_data[:,:,0],axis=0)\n",
    "std  = np.std(train_data[:,:,0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0_norm = (x_train0-mean)/std\n",
    "x_test0_norm = (x_test0-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_train_norm = []\n",
    "all_x_test_norm = []\n",
    "for isim in range(NSIM):\n",
    "    x_train_norm = (train_data[:,:,isim] - mean)/std\n",
    "    x_test_norm = (test_data[:,:,isim] - mean)/std\n",
    "    all_x_train_norm.append(x_train_norm) \n",
    "    all_x_test_norm.append(x_test_norm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Build a model\n",
    "About informations about : \n",
    " - [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    " - [Activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    " - [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    " - [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(shape):\n",
    "  \n",
    "  model = keras.models.Sequential()\n",
    "  model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    "  model.add(keras.layers.Dense(32, activation='relu', name='Dense_n1'))\n",
    "  model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "  model.add(keras.layers.Dense(32, activation='relu', name='Dense_n3'))\n",
    "  model.add(keras.layers.Dense(1, name='Output'))\n",
    "  \n",
    "  model.compile(optimizer = 'adam',\n",
    "                loss      = 'mse',\n",
    "                metrics   = ['mae', 'mse'] )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train the model\n",
    "### 5.1 - Get it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_n1 (Dense)            (None, 32)                224       \n",
      "                                                                 \n",
      " Dense_n2 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " Dense_n3 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,449\n",
      "Trainable params: 4,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "for isim in range(NSIM):\n",
    "    model=get_model_v1( (NBANDS,))\n",
    "    all_models.append(model)\n",
    "\n",
    "all_models[0].summary()\n",
    "\n",
    "# img=keras.utils.plot_model( model, to_file='./run/model.png', show_shapes=True, show_layer_names=True, dpi=96)\n",
    "# display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "476/476 [==============================] - 3s 4ms/step - loss: 0.1702 - mae: 0.2591 - mse: 0.1702 - val_loss: 0.0568 - val_mae: 0.1506 - val_mse: 0.0568\n",
      "Epoch 2/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0624 - mae: 0.1585 - mse: 0.0624 - val_loss: 0.0493 - val_mae: 0.1404 - val_mse: 0.0493\n",
      "Epoch 3/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0531 - mae: 0.1453 - mse: 0.0531 - val_loss: 0.0477 - val_mae: 0.1361 - val_mse: 0.0477\n",
      "Epoch 4/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0499 - mae: 0.1389 - mse: 0.0499 - val_loss: 0.0424 - val_mae: 0.1233 - val_mse: 0.0424\n",
      "Epoch 5/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0459 - mae: 0.1309 - mse: 0.0459 - val_loss: 0.0412 - val_mae: 0.1304 - val_mse: 0.0412\n",
      "Epoch 6/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0381 - mae: 0.1205 - mse: 0.0381 - val_loss: 0.0299 - val_mae: 0.1021 - val_mse: 0.0299\n",
      "Epoch 7/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0356 - mae: 0.1181 - mse: 0.0356 - val_loss: 0.0287 - val_mae: 0.1061 - val_mse: 0.0287\n",
      "Epoch 8/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0425 - mae: 0.1154 - mse: 0.0425 - val_loss: 0.0240 - val_mae: 0.0935 - val_mse: 0.0240\n",
      "Epoch 9/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0313 - mae: 0.1070 - mse: 0.0313 - val_loss: 0.0239 - val_mae: 0.0891 - val_mse: 0.0239\n",
      "Epoch 10/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0291 - mae: 0.1038 - mse: 0.0291 - val_loss: 0.0230 - val_mae: 0.0886 - val_mse: 0.0230\n",
      "Epoch 11/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0302 - mae: 0.1041 - mse: 0.0302 - val_loss: 0.0222 - val_mae: 0.0864 - val_mse: 0.0222\n",
      "Epoch 12/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0276 - mae: 0.1006 - mse: 0.0276 - val_loss: 0.0250 - val_mae: 0.0885 - val_mse: 0.0250\n",
      "Epoch 13/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0302 - mae: 0.1017 - mse: 0.0302 - val_loss: 0.0205 - val_mae: 0.0823 - val_mse: 0.0205\n",
      "Epoch 14/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0279 - mae: 0.0994 - mse: 0.0279 - val_loss: 0.0246 - val_mae: 0.0898 - val_mse: 0.0246\n",
      "Epoch 15/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0275 - mae: 0.0976 - mse: 0.0275 - val_loss: 0.0198 - val_mae: 0.0809 - val_mse: 0.0198\n",
      "Epoch 16/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0290 - mae: 0.0954 - mse: 0.0290 - val_loss: 0.0221 - val_mae: 0.0841 - val_mse: 0.0221\n",
      "Epoch 17/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0260 - mae: 0.0944 - mse: 0.0260 - val_loss: 0.0241 - val_mae: 0.0944 - val_mse: 0.0241\n",
      "Epoch 18/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0255 - mae: 0.0940 - mse: 0.0255 - val_loss: 0.0213 - val_mae: 0.0830 - val_mse: 0.0213\n",
      "Epoch 19/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0257 - mae: 0.0951 - mse: 0.0257 - val_loss: 0.0203 - val_mae: 0.0874 - val_mse: 0.0203\n",
      "Epoch 20/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0244 - mae: 0.0929 - mse: 0.0244 - val_loss: 0.0188 - val_mae: 0.0768 - val_mse: 0.0188\n",
      "Epoch 21/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0260 - mae: 0.0922 - mse: 0.0260 - val_loss: 0.0224 - val_mae: 0.0935 - val_mse: 0.0224\n",
      "Epoch 22/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0247 - mae: 0.0920 - mse: 0.0247 - val_loss: 0.0190 - val_mae: 0.0781 - val_mse: 0.0190\n",
      "Epoch 23/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0242 - mae: 0.0909 - mse: 0.0242 - val_loss: 0.0207 - val_mae: 0.0840 - val_mse: 0.0207\n",
      "Epoch 24/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0251 - mae: 0.0912 - mse: 0.0251 - val_loss: 0.0195 - val_mae: 0.0786 - val_mse: 0.0195\n",
      "Epoch 25/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0230 - mae: 0.0886 - mse: 0.0230 - val_loss: 0.0201 - val_mae: 0.0801 - val_mse: 0.0201\n",
      "Epoch 26/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0250 - mae: 0.0912 - mse: 0.0250 - val_loss: 0.0192 - val_mae: 0.0749 - val_mse: 0.0192\n",
      "Epoch 27/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0234 - mae: 0.0886 - mse: 0.0234 - val_loss: 0.0193 - val_mae: 0.0801 - val_mse: 0.0193\n",
      "Epoch 28/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0226 - mae: 0.0877 - mse: 0.0226 - val_loss: 0.0190 - val_mae: 0.0752 - val_mse: 0.0190\n",
      "Epoch 29/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0227 - mae: 0.0871 - mse: 0.0227 - val_loss: 0.0195 - val_mae: 0.0757 - val_mse: 0.0195\n",
      "Epoch 30/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0236 - mae: 0.0893 - mse: 0.0236 - val_loss: 0.0193 - val_mae: 0.0766 - val_mse: 0.0193\n",
      "Epoch 31/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0233 - mae: 0.0892 - mse: 0.0233 - val_loss: 0.0182 - val_mae: 0.0764 - val_mse: 0.0182\n",
      "Epoch 32/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.0874 - mse: 0.0228 - val_loss: 0.0188 - val_mae: 0.0766 - val_mse: 0.0188\n",
      "Epoch 33/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0234 - mae: 0.0877 - mse: 0.0234 - val_loss: 0.0200 - val_mae: 0.0852 - val_mse: 0.0200\n",
      "Epoch 34/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0220 - mae: 0.0857 - mse: 0.0220 - val_loss: 0.0173 - val_mae: 0.0712 - val_mse: 0.0173\n",
      "Epoch 35/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0219 - mae: 0.0853 - mse: 0.0219 - val_loss: 0.0198 - val_mae: 0.0796 - val_mse: 0.0198\n",
      "Epoch 36/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.0859 - mse: 0.0221 - val_loss: 0.0186 - val_mae: 0.0733 - val_mse: 0.0186\n",
      "Epoch 37/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0215 - mae: 0.0849 - mse: 0.0215 - val_loss: 0.0170 - val_mae: 0.0725 - val_mse: 0.0170\n",
      "Epoch 38/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0214 - mae: 0.0846 - mse: 0.0214 - val_loss: 0.0182 - val_mae: 0.0776 - val_mse: 0.0182\n",
      "Epoch 39/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0218 - mae: 0.0841 - mse: 0.0218 - val_loss: 0.0170 - val_mae: 0.0708 - val_mse: 0.0170\n",
      "Epoch 40/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0208 - mae: 0.0834 - mse: 0.0208 - val_loss: 0.0202 - val_mae: 0.0752 - val_mse: 0.0202\n",
      "Epoch 41/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0220 - mae: 0.0850 - mse: 0.0220 - val_loss: 0.0172 - val_mae: 0.0696 - val_mse: 0.0172\n",
      "Epoch 42/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0213 - mae: 0.0835 - mse: 0.0213 - val_loss: 0.0175 - val_mae: 0.0713 - val_mse: 0.0175\n",
      "Epoch 43/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0211 - mae: 0.0828 - mse: 0.0211 - val_loss: 0.0173 - val_mae: 0.0753 - val_mse: 0.0173\n",
      "Epoch 44/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0206 - mae: 0.0829 - mse: 0.0206 - val_loss: 0.0195 - val_mae: 0.0765 - val_mse: 0.0195\n",
      "Epoch 45/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0210 - mae: 0.0825 - mse: 0.0210 - val_loss: 0.0174 - val_mae: 0.0734 - val_mse: 0.0174\n",
      "Epoch 46/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0212 - mae: 0.0843 - mse: 0.0212 - val_loss: 0.0187 - val_mae: 0.0743 - val_mse: 0.0187\n",
      "Epoch 47/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0212 - mae: 0.0816 - mse: 0.0212 - val_loss: 0.0199 - val_mae: 0.0773 - val_mse: 0.0199\n",
      "Epoch 48/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0200 - mae: 0.0812 - mse: 0.0200 - val_loss: 0.0176 - val_mae: 0.0691 - val_mse: 0.0176\n",
      "Epoch 49/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0200 - mae: 0.0816 - mse: 0.0200 - val_loss: 0.0170 - val_mae: 0.0677 - val_mse: 0.0170\n",
      "Epoch 50/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0204 - mae: 0.0809 - mse: 0.0204 - val_loss: 0.0163 - val_mae: 0.0680 - val_mse: 0.0163\n",
      "Epoch 1/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.1208 - mae: 0.2209 - mse: 0.1208 - val_loss: 0.0568 - val_mae: 0.1526 - val_mse: 0.0568\n",
      "Epoch 2/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0600 - mae: 0.1569 - mse: 0.0600 - val_loss: 0.0492 - val_mae: 0.1389 - val_mse: 0.0492\n",
      "Epoch 3/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0567 - mae: 0.1473 - mse: 0.0567 - val_loss: 0.0450 - val_mae: 0.1284 - val_mse: 0.0450\n",
      "Epoch 4/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0499 - mae: 0.1403 - mse: 0.0499 - val_loss: 0.0404 - val_mae: 0.1309 - val_mse: 0.0404\n",
      "Epoch 5/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0444 - mae: 0.1326 - mse: 0.0444 - val_loss: 0.0362 - val_mae: 0.1158 - val_mse: 0.0362\n",
      "Epoch 6/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0408 - mae: 0.1292 - mse: 0.0408 - val_loss: 0.0362 - val_mae: 0.1105 - val_mse: 0.0362\n",
      "Epoch 7/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0354 - mae: 0.1191 - mse: 0.0354 - val_loss: 0.0300 - val_mae: 0.1066 - val_mse: 0.0300\n",
      "Epoch 8/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0350 - mae: 0.1162 - mse: 0.0350 - val_loss: 0.0264 - val_mae: 0.0971 - val_mse: 0.0264\n",
      "Epoch 9/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0330 - mae: 0.1135 - mse: 0.0330 - val_loss: 0.0267 - val_mae: 0.1012 - val_mse: 0.0267\n",
      "Epoch 10/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0290 - mae: 0.1067 - mse: 0.0290 - val_loss: 0.0238 - val_mae: 0.0925 - val_mse: 0.0238\n",
      "Epoch 11/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0289 - mae: 0.1050 - mse: 0.0289 - val_loss: 0.0282 - val_mae: 0.1045 - val_mse: 0.0282\n",
      "Epoch 12/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0275 - mae: 0.1016 - mse: 0.0275 - val_loss: 0.0213 - val_mae: 0.0856 - val_mse: 0.0213\n",
      "Epoch 13/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0277 - mae: 0.1014 - mse: 0.0277 - val_loss: 0.0236 - val_mae: 0.0930 - val_mse: 0.0236\n",
      "Epoch 14/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0272 - mae: 0.0993 - mse: 0.0272 - val_loss: 0.0237 - val_mae: 0.0869 - val_mse: 0.0237\n",
      "Epoch 15/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0259 - mae: 0.0970 - mse: 0.0259 - val_loss: 0.0224 - val_mae: 0.0928 - val_mse: 0.0224\n",
      "Epoch 16/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0256 - mae: 0.0959 - mse: 0.0256 - val_loss: 0.0201 - val_mae: 0.0850 - val_mse: 0.0201\n",
      "Epoch 17/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0246 - mae: 0.0939 - mse: 0.0246 - val_loss: 0.0218 - val_mae: 0.0811 - val_mse: 0.0218\n",
      "Epoch 18/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0243 - mae: 0.0918 - mse: 0.0243 - val_loss: 0.0195 - val_mae: 0.0811 - val_mse: 0.0195\n",
      "Epoch 19/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0240 - mae: 0.0924 - mse: 0.0240 - val_loss: 0.0199 - val_mae: 0.0841 - val_mse: 0.0199\n",
      "Epoch 20/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0239 - mae: 0.0911 - mse: 0.0239 - val_loss: 0.0188 - val_mae: 0.0763 - val_mse: 0.0188\n",
      "Epoch 21/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0233 - mae: 0.0914 - mse: 0.0233 - val_loss: 0.0218 - val_mae: 0.0892 - val_mse: 0.0218\n",
      "Epoch 22/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0236 - mae: 0.0909 - mse: 0.0236 - val_loss: 0.0244 - val_mae: 0.0892 - val_mse: 0.0244\n",
      "Epoch 23/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0236 - mae: 0.0903 - mse: 0.0236 - val_loss: 0.0188 - val_mae: 0.0771 - val_mse: 0.0188\n",
      "Epoch 24/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0232 - mae: 0.0887 - mse: 0.0232 - val_loss: 0.0202 - val_mae: 0.0835 - val_mse: 0.0202\n",
      "Epoch 25/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0228 - mae: 0.0883 - mse: 0.0228 - val_loss: 0.0176 - val_mae: 0.0740 - val_mse: 0.0176\n",
      "Epoch 26/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0221 - mae: 0.0867 - mse: 0.0221 - val_loss: 0.0180 - val_mae: 0.0744 - val_mse: 0.0180\n",
      "Epoch 27/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.0863 - mse: 0.0220 - val_loss: 0.0179 - val_mae: 0.0760 - val_mse: 0.0179\n",
      "Epoch 28/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0220 - mae: 0.0849 - mse: 0.0220 - val_loss: 0.0175 - val_mae: 0.0746 - val_mse: 0.0175\n",
      "Epoch 29/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0215 - mae: 0.0852 - mse: 0.0215 - val_loss: 0.0200 - val_mae: 0.0729 - val_mse: 0.0200\n",
      "Epoch 30/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0219 - mae: 0.0862 - mse: 0.0219 - val_loss: 0.0166 - val_mae: 0.0716 - val_mse: 0.0166\n",
      "Epoch 31/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0207 - mae: 0.0848 - mse: 0.0207 - val_loss: 0.0175 - val_mae: 0.0711 - val_mse: 0.0175\n",
      "Epoch 32/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.0855 - mse: 0.0222 - val_loss: 0.0183 - val_mae: 0.0770 - val_mse: 0.0183\n",
      "Epoch 33/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0218 - mae: 0.0843 - mse: 0.0218 - val_loss: 0.0174 - val_mae: 0.0716 - val_mse: 0.0174\n",
      "Epoch 34/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0205 - mae: 0.0831 - mse: 0.0205 - val_loss: 0.0182 - val_mae: 0.0755 - val_mse: 0.0182\n",
      "Epoch 35/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0210 - mae: 0.0837 - mse: 0.0210 - val_loss: 0.0168 - val_mae: 0.0704 - val_mse: 0.0168\n",
      "Epoch 36/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0207 - mae: 0.0839 - mse: 0.0207 - val_loss: 0.0182 - val_mae: 0.0777 - val_mse: 0.0182\n",
      "Epoch 37/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0204 - mae: 0.0829 - mse: 0.0204 - val_loss: 0.0178 - val_mae: 0.0693 - val_mse: 0.0178\n",
      "Epoch 38/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0204 - mae: 0.0834 - mse: 0.0204 - val_loss: 0.0189 - val_mae: 0.0718 - val_mse: 0.0189\n",
      "Epoch 39/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0195 - mae: 0.0810 - mse: 0.0195 - val_loss: 0.0173 - val_mae: 0.0701 - val_mse: 0.0173\n",
      "Epoch 40/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0197 - mae: 0.0827 - mse: 0.0197 - val_loss: 0.0177 - val_mae: 0.0793 - val_mse: 0.0177\n",
      "Epoch 41/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0201 - mae: 0.0815 - mse: 0.0201 - val_loss: 0.0179 - val_mae: 0.0741 - val_mse: 0.0179\n",
      "Epoch 42/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0200 - mae: 0.0812 - mse: 0.0200 - val_loss: 0.0194 - val_mae: 0.0745 - val_mse: 0.0194\n",
      "Epoch 43/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0205 - mae: 0.0832 - mse: 0.0205 - val_loss: 0.0158 - val_mae: 0.0672 - val_mse: 0.0158\n",
      "Epoch 44/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0192 - mae: 0.0800 - mse: 0.0192 - val_loss: 0.0163 - val_mae: 0.0700 - val_mse: 0.0163\n",
      "Epoch 45/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0200 - mae: 0.0814 - mse: 0.0200 - val_loss: 0.0166 - val_mae: 0.0706 - val_mse: 0.0166\n",
      "Epoch 46/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0190 - mae: 0.0797 - mse: 0.0190 - val_loss: 0.0180 - val_mae: 0.0741 - val_mse: 0.0180\n",
      "Epoch 47/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0197 - mae: 0.0814 - mse: 0.0197 - val_loss: 0.0155 - val_mae: 0.0677 - val_mse: 0.0155\n",
      "Epoch 48/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0182 - mae: 0.0782 - mse: 0.0182 - val_loss: 0.0203 - val_mae: 0.0743 - val_mse: 0.0203\n",
      "Epoch 49/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0201 - mae: 0.0815 - mse: 0.0201 - val_loss: 0.0168 - val_mae: 0.0676 - val_mse: 0.0168\n",
      "Epoch 50/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0197 - mae: 0.0805 - mse: 0.0197 - val_loss: 0.0163 - val_mae: 0.0678 - val_mse: 0.0163\n",
      "Epoch 1/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.1248 - mae: 0.2341 - mse: 0.1248 - val_loss: 0.0552 - val_mae: 0.1520 - val_mse: 0.0552\n",
      "Epoch 2/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0579 - mae: 0.1525 - mse: 0.0579 - val_loss: 0.0504 - val_mae: 0.1444 - val_mse: 0.0504\n",
      "Epoch 3/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0521 - mae: 0.1430 - mse: 0.0521 - val_loss: 0.0424 - val_mae: 0.1241 - val_mse: 0.0424\n",
      "Epoch 4/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0458 - mae: 0.1339 - mse: 0.0458 - val_loss: 0.0364 - val_mae: 0.1182 - val_mse: 0.0364\n",
      "Epoch 5/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0406 - mae: 0.1265 - mse: 0.0406 - val_loss: 0.0344 - val_mae: 0.1204 - val_mse: 0.0344\n",
      "Epoch 6/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0369 - mae: 0.1203 - mse: 0.0369 - val_loss: 0.0304 - val_mae: 0.1134 - val_mse: 0.0304\n",
      "Epoch 7/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0372 - mae: 0.1174 - mse: 0.0372 - val_loss: 0.0274 - val_mae: 0.0998 - val_mse: 0.0274\n",
      "Epoch 8/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0322 - mae: 0.1114 - mse: 0.0322 - val_loss: 0.0240 - val_mae: 0.0958 - val_mse: 0.0240\n",
      "Epoch 9/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0309 - mae: 0.1075 - mse: 0.0309 - val_loss: 0.0229 - val_mae: 0.0899 - val_mse: 0.0229\n",
      "Epoch 10/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0295 - mae: 0.1045 - mse: 0.0295 - val_loss: 0.0229 - val_mae: 0.0912 - val_mse: 0.0229\n",
      "Epoch 11/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0297 - mae: 0.1026 - mse: 0.0297 - val_loss: 0.0214 - val_mae: 0.0869 - val_mse: 0.0214\n",
      "Epoch 12/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0280 - mae: 0.1014 - mse: 0.0280 - val_loss: 0.0248 - val_mae: 0.0895 - val_mse: 0.0248\n",
      "Epoch 13/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0283 - mae: 0.1008 - mse: 0.0283 - val_loss: 0.0220 - val_mae: 0.0889 - val_mse: 0.0220\n",
      "Epoch 14/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0273 - mae: 0.0993 - mse: 0.0273 - val_loss: 0.0208 - val_mae: 0.0873 - val_mse: 0.0208\n",
      "Epoch 15/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0268 - mae: 0.0990 - mse: 0.0268 - val_loss: 0.0209 - val_mae: 0.0865 - val_mse: 0.0209\n",
      "Epoch 16/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0256 - mae: 0.0952 - mse: 0.0256 - val_loss: 0.0230 - val_mae: 0.0909 - val_mse: 0.0230\n",
      "Epoch 17/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0264 - mae: 0.0965 - mse: 0.0264 - val_loss: 0.0201 - val_mae: 0.0805 - val_mse: 0.0201\n",
      "Epoch 18/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0255 - mae: 0.0955 - mse: 0.0255 - val_loss: 0.0206 - val_mae: 0.0846 - val_mse: 0.0206\n",
      "Epoch 19/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0257 - mae: 0.0946 - mse: 0.0257 - val_loss: 0.0199 - val_mae: 0.0801 - val_mse: 0.0199\n",
      "Epoch 20/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0248 - mae: 0.0922 - mse: 0.0248 - val_loss: 0.0228 - val_mae: 0.0965 - val_mse: 0.0228\n",
      "Epoch 21/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0244 - mae: 0.0923 - mse: 0.0244 - val_loss: 0.0182 - val_mae: 0.0770 - val_mse: 0.0182\n",
      "Epoch 22/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0242 - mae: 0.0908 - mse: 0.0242 - val_loss: 0.0182 - val_mae: 0.0760 - val_mse: 0.0182\n",
      "Epoch 23/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0247 - mae: 0.0921 - mse: 0.0247 - val_loss: 0.0246 - val_mae: 0.0934 - val_mse: 0.0246\n",
      "Epoch 24/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0241 - mae: 0.0919 - mse: 0.0241 - val_loss: 0.0195 - val_mae: 0.0775 - val_mse: 0.0195\n",
      "Epoch 25/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0231 - mae: 0.0879 - mse: 0.0231 - val_loss: 0.0207 - val_mae: 0.0785 - val_mse: 0.0207\n",
      "Epoch 26/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0233 - mae: 0.0902 - mse: 0.0233 - val_loss: 0.0181 - val_mae: 0.0752 - val_mse: 0.0181\n",
      "Epoch 27/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0236 - mae: 0.0889 - mse: 0.0236 - val_loss: 0.0196 - val_mae: 0.0781 - val_mse: 0.0196\n",
      "Epoch 28/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0230 - mae: 0.0901 - mse: 0.0230 - val_loss: 0.0181 - val_mae: 0.0766 - val_mse: 0.0181\n",
      "Epoch 29/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0229 - mae: 0.0874 - mse: 0.0229 - val_loss: 0.0181 - val_mae: 0.0764 - val_mse: 0.0181\n",
      "Epoch 30/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0225 - mae: 0.0876 - mse: 0.0225 - val_loss: 0.0202 - val_mae: 0.0783 - val_mse: 0.0202\n",
      "Epoch 31/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0216 - mae: 0.0860 - mse: 0.0216 - val_loss: 0.0181 - val_mae: 0.0774 - val_mse: 0.0181\n",
      "Epoch 32/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0222 - mae: 0.0871 - mse: 0.0222 - val_loss: 0.0192 - val_mae: 0.0760 - val_mse: 0.0192\n",
      "Epoch 33/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0228 - mae: 0.0877 - mse: 0.0228 - val_loss: 0.0193 - val_mae: 0.0860 - val_mse: 0.0193\n",
      "Epoch 34/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0222 - mae: 0.0861 - mse: 0.0222 - val_loss: 0.0182 - val_mae: 0.0760 - val_mse: 0.0182\n",
      "Epoch 35/50\n",
      "476/476 [==============================] - 1s 3ms/step - loss: 0.0214 - mae: 0.0840 - mse: 0.0214 - val_loss: 0.0183 - val_mae: 0.0743 - val_mse: 0.0183\n",
      "Epoch 36/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0221 - mae: 0.0858 - mse: 0.0221 - val_loss: 0.0192 - val_mae: 0.0742 - val_mse: 0.0192\n",
      "Epoch 37/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0208 - mae: 0.0821 - mse: 0.0208 - val_loss: 0.0181 - val_mae: 0.0726 - val_mse: 0.0181\n",
      "Epoch 38/50\n",
      "476/476 [==============================] - 2s 4ms/step - loss: 0.0214 - mae: 0.0847 - mse: 0.0214 - val_loss: 0.0228 - val_mae: 0.0775 - val_mse: 0.0228\n",
      "Epoch 39/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0216 - mae: 0.0835 - mse: 0.0216 - val_loss: 0.0209 - val_mae: 0.0739 - val_mse: 0.0209\n",
      "Epoch 40/50\n",
      "476/476 [==============================] - 2s 3ms/step - loss: 0.0210 - mae: 0.0837 - mse: 0.0210 - val_loss: 0.0177 - val_mae: 0.0714 - val_mse: 0.0177\n",
      "Epoch 41/50\n",
      "454/476 [===========================>..] - ETA: 0s - loss: 0.0213 - mae: 0.0831 - mse: 0.0213"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m isim,the_model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_models):\n\u001b[1;32m      3\u001b[0m     x_train \u001b[38;5;241m=\u001b[39m all_x_train_norm[isim] \n\u001b[0;32m----> 4\u001b[0m     the_history \u001b[38;5;241m=\u001b[39m \u001b[43mthe_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfit_verbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test0_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     all_history\u001b[38;5;241m.\u001b[39mappend(the_history)\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/keras/engine/training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1716\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1717\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1728\u001b[0m     )\n\u001b[0;32m-> 1729\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1744\u001b[0m }\n\u001b[1;32m   1745\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/keras/engine/training.py:2072\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2070\u001b[0m ):\n\u001b[1;32m   2071\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2072\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   2074\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    935\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/fidle23/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_history = []\n",
    "for isim,the_model in enumerate(all_models):\n",
    "    x_train = all_x_train_norm[isim] \n",
    "    the_history = the_model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs          = 50,\n",
    "                    batch_size      = 20,\n",
    "                    verbose         = fit_verbosity,\n",
    "                    validation_data = (x_test0_norm, y_test))\n",
    "    all_history.append(the_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6 - Evaluate\n",
    "### 6.1 - Model evaluation\n",
    "MAE =  Mean Absolute Error (between the labels and predictions)  \n",
    "A mae equal to 3 represents an average error in prediction of $3k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Training history\n",
    "What was the best result during our training ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=history.history)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.scrawler.history( history, plot={'MSE' :['mse', 'val_mse'],\n",
    "                        'MAE' :['mae', 'val_mae'],\n",
    "                        'LOSS':['loss','val_loss']}, save_as='01-history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Make a prediction\n",
    "The data must be normalized with the parameters (mean, std) previously used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data=x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict( my_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_spec = y_test\n",
    "z_phot = predictions.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt = get_stats(z_spec,z_phot,slope=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are matplotlib.patch.Patch properties\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(zmin=0,zmax=3,zstep=0.05,slope=0.15):\n",
    "    x = np.arange(zmin,zmax,zstep)\n",
    "    outlier_upper = x + slope*(1+x)\n",
    "    outlier_lower = x - slope*(1+x)\n",
    "    return x,outlier_upper,outlier_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,outlier_upper,outlier_lower = plot_lines(zmin=0,zmax=3,zstep=0.05,slope=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,10))\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "ax.scatter(z_spec,z_phot,marker='.',c=\"b\",s=30,alpha=0.1)\n",
    "ax.text(0.05, 0.95, stats_txt, transform=ax.transAxes, fontsize=14,\n",
    "        verticalalignment='top', bbox=props)\n",
    "ax.set_xlabel(\"$z_{spec}$ (true target value)\")\n",
    "ax.set_ylabel(\"$z_{phot}$ (predicted target value)\")\n",
    "ax.set_title(\"DNN :photometric redshift vs spectroscopic redshift\")\n",
    "ax.plot(x,outlier_upper,'k:')\n",
    "ax.plot(x,outlier_lower,'k:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidle.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fidle23",
   "language": "python",
   "name": "fidle23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3929042cc22c1274d74e3e946c52b845b57cb6d84f2d591ffe0519b38e4896d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
